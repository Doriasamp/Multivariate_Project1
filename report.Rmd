---
title: "MA369 Group Project #1"
author: "Andrea Marchi, Tristan Rumsey, Joey Scheuer, Rachel Hughes"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
setecho=T
knitr::opts_chunk$set(echo = setecho)
```


### Title Page and Individual Contributions
```{r}
#Part a - Lead: Andrea Marchi, Reviewer: Rachel Hughes
#Part b - Lead: Rachel Hughes, Reviewer: Andrea Marchi
#Part c - Lead: Andrea Marchi, Reviewer: Joey Scheuer
#Part d - Lead: Joey Scheuer, Reviewer: Andrea Marchi
#Part e - Lead: Joey Scheuer, Reviewer: Tristan Rumsey
#Part f - Lead: Tristan Rumsey, Reviewer: Rachel Hughes
#Part g - Lead: Rachel Hughes, Reviewer: Joey Scheuer
#Part h - Lead: Tristan Rumsey, Reviewer: Rachel Hughes
#Part i - Lead: Tristan Rumsey, Reviewer: Andrea Marchi
#Part j - Lead: Joey Scheuer, Reviewer: Rachel Hughes
#Part k - Lead: Andrea Marchi, Reviewer: Tristan Rumsey




```


#Install needed dependencies
```{r, dependencies, echo=setecho}
# First thing you have to installing the following dependencies within library()
# and you only have to do this once
suppressWarnings({
  library(haven)
  library(readr)
  suppressMessages({library(GGally)})
})
```


## Part a
```{r, part_a, echo=setecho}
url = "https://stats.idre.ucla.edu/wp-content/uploads/2016/02/crime.sas7bdat"
crime_import = read_sas(url, NULL)
str(crime_import)
```
#### Answers and comments:
The dataset has a total of 51 observations, and a total of 9 variables. Of the 9 variables, 8 of them are numeric, and 1 is categorical (state).


## Part b
```{r, part_b, echo=setecho}
write.csv(crime_import, file = "crime_csv.csv", row.names = F)
write.table(crime_import, file = "crime_txt.txt", sep = "\t", row.names = F)
dir()
```
#### Answers and comments:
As seen above, the current working directory has a total of 7 different items, including the csv file of the data that we just created, titled "crime_csv", as well as the text file titled "crime_txt".


## Part c
```{r, part_c, echo=setecho}
crime_numeric = read_csv("crime_csv.csv",
                         col_types = cols(sid = col_skip(),
                                          state = col_skip(),
                                          crime = col_skip()))

str(crime_numeric)
```
#### Answers and comments:
We have removed the three variables called sid (state id), state, and crime, and assigned the remaining 6 variables to the "crime_numeric" dataframe. In the structure of this dataframe, there are 51 observations, and a total of 6 variables, all of which are numeric variables.


## Part d
```{r, part_d, echo=setecho}
boxplot(crime_numeric, las = 2, col = rainbow(ncol(crime_numeric)))
```
#### Answers and comments:
In the above boxplot, all 6 variables can be visualized in a different color. You can identify outliers for several of the variables. This includes 2 outliers for the "murder" variable, 2 outliers for the "pctwhite" variable, and 1 outlier for the "single" variable


## Part e
```{r, part_e, echo=setecho}
ggpairs(crime_numeric)
```
#### Answers and comments:
#Murder variable:
The murder variable has a heavily right-skewed distribution. 
#Pctmetro variable:
The pctmetro variable has a left-skewed distribution.
#Pctwhite variable:
The pctwhite variable has a heavily left-skewed distribution.
#Pcths variable:
The pcths variable has a relatively normal distribution.
#Poverty variable:
The poverty variable has a right-skewed distribution.
#Single variable:
The single variable has a heavily right-skewed distribution.
#Murder and pctmetro variables:
The murder variable has a weak positive association with pctmetro, as the correlation coefficient, r, equals 0.316, which is confirmed by the scatterplot that shows a positive slope.
#Murder and pctwhite variables:
The murder variable has a moderately strong negative association with pctwhite variable, as r = -0.706. This strong correlation is confirmed by the scatterplot that shows a strong relationship.
#Murder and pcths variables:
The murder variable has a moderately weak negative association with the pcths variable, as r = -0.286. This is confirmed by the scatterplot that shows a weak relationship.
#Murder and poverty variables:
The murder variable has a moderately strong positive association with the poverty variable, as r = 0.566. This is confirmed by the scatterplot that shows a moderately strong relationship.
#Murder and single variables:
The murder variable has a strong positive association with the single variable, as r = 0.859. This is confirmed by the scatterplot that shows a strong relationship.
#Pctmetro and pctwhite variables:
The pctmetro variable has a moderately weak negative association with the pctwhite variable, as r = -0.337. This is confirmed by the scatterplot that shows a moderately weak relationship.
#Pctmetro and pcths variables:
The pctmetro variable has a no association with the pcths variable, as r = -0.004. This is confirmed by the scatterplot that shows virtually no relationship.
#Pctmetro and poverty variables:
The pctmetro variable has a very weak negative association with the poverty variable, as r = -0.061. This is confirmed by the scatterplot that shows a very weak relationship.
#Pctmetro and single variables:
The pctmetro variable has a moderately weak positive association with the poverty variable, as r = 0.260. This is confirmed by the scatterplot that shows a moderately weak relationship.
#Pctwhite and pcths variables:
The pctwhite variable has a moderately weak positive association with the pcths variable, as r = 0.339. This is confirmed by the scatterplot that shows a moderately weak relationship.
#Pctwhite and poverty variables:
The pctwhite variable has a moderately weak negative association with the poverty variable, as r = -0.389. This is confirmed by the scatterplot that shows a moderately weak relationship.
#Pctwhite and single variables:
The pctwhite variable has a moderately strong negative association with the poverty variable, as r = -0.656. This is confirmed by the scatterplot that shows a moderately strong relationship.
#Pcths and poverty variables:
The pcths variable has a strong negative association with the poverty variable, as r = -0.744. This is confirmed by the scatterplot that shows a strong relationship.
#Pcths and single variables:
The pcths variable has a weak negative association with the single variable, as r = -0.220. This is confirmed by the scatterplot that shows a weak relationship.
#Poverty and single variables:
The poverty variable has a moderately strong positive association with the single variable, as r = 0.549. This is confirmed by the scatterplot that shows a moderately strong relationship.


## Part f
```{r, part_f, echo=setecho}
#converting data into dataframe and into matrix
crime_df = data.frame(crime_numeric)
crime_matrix = data.matrix(crime_df)
crime_matrix

#sample covariance matrix
S = round(cov(crime_matrix), 2)
S

#sample correlation matrix
R = round(cor(crime_matrix), 2)
R

#standardized data matrix
std.crime_mat = scale(crime_matrix)

#standardized sample covariance matrix
S.std = round(cov(std.crime_mat), 2)
S.std
```
#### Answers and comments:
Note that the sample correlation matrix is already standardized as it is equal to the standardized sample covariance matrix. Also note the high correlation between murder with pctmetro, poverty, and single versus the strong negative correlation between murder with pcths, and pctwhite. These same relationships are shown through the covariance matrix also.


## Part g
```{r, part_g, echo=setecho}
GSV = det(S)
GSV

#inverse of sample covariance matrix
S.inv = round(solve(S), 3)
S.inv
```
#### Answers and comments:
Since the GSV does not equal zero (GSV = 649,502,892), S is invertible. Since this is a very large number, it shows that the variation in the data is high (the data is widely dispersed). 
The inverse covariance matrix once again shows the correlations between murder and other variables. Murder with pctwhite and pctmetro are showing very low correlation while murder with single is showing a strong correlation.


## Part h
```{r, part_h, echo=setecho}
#spectral decomposition
Eig.S = eigen(S)
EigVals.S = round(Eig.S$values, 2)
EigVals.S #getting the eigenvalues

#obtaining the eigenvector matrix (P)
P=round(Eig.S$vectors, 2)
P

#showing that P'P = PP' = I
round(P %*% t(P), 2)

#obtaining the lambda matrix (D)
D=diag(EigVals.S)
D

#compute the spectral decomposition matrix, PDP'
Cov.Spec.Decomp = round(P %*% D %*% t(P), 2)
Cov.Spec.Decomp
S

#calculating determinant and inverse of covariance matrix using spectral decomposition
det.S.decomp = prod(EigVals.S)
det.S.decomp

#calculating lambda inverse matrix (D inverse)
D.inv = round(diag(1/EigVals.S), 2)
D.inv

#calculating inverse covariance matrix
S.inv.decomp = P %*% D.inv %*% t(P)
S.inv.decomp

```
#### Answers and comments:
We can see that creating P by combining the eigenvectors and multiplying that P' results in the identity matrix. When comparing the original covariance matrix and the spectral decomposition matrix we can see that the values of the two matrices are very close. When we calculate the determinant in this step we get 650566462 which is very close to the value of 650566462 that we get when we calculate the determinant in step g. 
A matrix is a PD matrix if all of its eigenvalues > 0. As seen in the beginning of part h, in calculating the eigenvalues, we found eigenvalues of 536.70, 210.98, 43.62, 33.20, 4.09, and 0.97. All of these eigenvalues are greater than 0. Because of the, we can say that the covariance matrix (S) is a positive definite (PD) matrix.


## Part i
```{r, part_i, echo=setecho}
#get the inverse square root of the covariance matrix (D^-1/2)
D.inv.sqrt = round(diag(sqrt(1/EigVals.S)),2)
D.inv.sqrt

#multiply the "data matrix" and inverse square root of the covariance matrix
mult_matrix = round(crime_matrix %*% D.inv.sqrt, 2)

#compute covariance matrix of the scaled data
S.scaled = round(cov(mult_matrix), 2)
S.scaled

```
#### Answers and comments:
The covariance matrix of the scaled data shows the same signs as the covariance matrix of the standardized data. While the values are different, the matrices show the positive or negative relationship between each pair of variables.


## Part j
```{r, part_j, echo=setecho}
#simulate and print the random sample
set.seed(123)
cat("Random vector with n = # feature in crime_mat = 6:\n")
x_vec = rnorm(dim(crime_df)[2], mean=mean(crime_matrix), sd=sd(crime_matrix))
x_vec

#show S is a PD matrix (x' %*% S %*% x)
t(x_vec) %*% S %*% x_vec
```
#### Answers and comments:
We simulated from a normal distribution a random sample of size equal to the number of features in crime_mat, and its corresponding sample mean and and standard deviation, but before we set the random seed to 123, to allow us to reproduce the output in different runtime.
Finally, we named this random vector X and we showed that X'SX it returns a very large positive value (616.6908) as expected for a positive definite matrix. In fact, we could theoretically repeat the experiment with all the possible (infinite) combinations of such a vector and still get a positive value.



## Part k
```{r, part_k, echo=setecho}
# Creating the SVD matrix
SVD_matrix = svd(crime_matrix)
# Creating the U matrix
U = SVD_matrix$u
# Creating the V matrix
V = SVD_matrix$v
# Creating and printing the lambda matrix
lambda = diag(SVD_matrix$d)
cat("Lambda Matrix from SVD:\n\n")
lambda
# Checking and printing the dimensions of U, V, and Lambda
cat(paste("\nDimension of U:", paste(dim(U), collapse = "x")), "\n")
cat(paste("Dimension of V:", paste(dim(V), collapse = "x")), "\n")
cat(paste("Dimension of Lambda:", paste(dim(lambda), collapse = "x")), "\n")
# Reconstructing original crime_mat data with UDV'
X = U%*%lambda%*%t(V)
# Showing how this reconstruction it returns the original data (-with only floating point errors)
cat("\nTotal overall difference between original crime_mat data and reconstructed data matrix with SVD: ",
    max(abs(crime_matrix-X)))
r = dim(lambda)[1]-2
lambda_r = lambda[1:r, 1:r]
cat(paste("\n\nNew dimension of Lambda:", paste(dim(lambda_r), collapse = "x")))
U_r = U[, 1:r]
V_r = V[, 1:r]
X_reduced_reconstruct = U_r %*% lambda_r %*% t(V_r)
cat("\n\nTotal overall difference between original crime_mat data and reconstructed and p-2 reduced data matrix with SVD: ", max(abs(crime_matrix-X_reduced_reconstruct)))
```
#### Answers and comments:
When computing the singular value decomposition on our crime_mat matrix we can notice that from lambda matrix printout, none of the values is 0 zero or close to zero. Because of that we can infer that none of the variables in crime_mat is a linear combination of other variables, and we should that reducing the rank it will cause a loss of important features
The dimension of matrices U, V and lambda from SVD are respectively 51 by 6, 6 by 6, and 6 by 6.
Moreover, by taking advantage of SVD we proved that the identity X = UDV' (where X is the original data matrix) holds true. In fact, by computing the absolute value of the difference between crime_mat and the matrix obtained by multiplying U, D and V', we obtain a value that is basically zero.
Finally, we wanted to show that by reducing the rank by 2, with features that as showed by the printout of lambda have a non-zero value, we cannot obtain our original crime_mat matrix by UDV' identity.
In showing this we reduced the rank by eliminating the 2 columns with lowest eigenvalues in main diagonal, and by computing the absolute difference between crime_mat and UDV' with reduced rank we get a value of 25.75, showing that the matrix reconstructed with reduced rank without non-zero eigenvalues is different then crime_mat.






