---
title: "MA369 Group Project #1"
author: "Andrea Marchi, Tristan Rumsey, Joey Scheuer, Rachel Hughes"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
setecho=T
knitr::opts_chunk$set(echo = setecho)
```


### Title Page and Individual Contributions
```{r}
#Part a - Lead: Andrea Marchi, Reviewer: Rachel Hughes
#Part b - Lead: Andrea Marchi, Reviewer: Rachel Hughes
#Part c - Lead: Andrea Marchi, Reviewer: Joey Scheuer
#Part d - Lead: Andrea Marchi, Reviewer: Joey Scheuer
#Part e - Lead: Andrea Marchi, Reviewer: Joey Scheuer
#Part f - Lead: Tristan Rumsey, Reviewer: Joey Scheuer
#Part g - Lead: Tristan Rumsey, Reviewer: Joey Scheuer
#Part h - Lead: Tristan Rumsey, Reviewer: Joey Scheuer
#Part i - Lead: Tristan Rumsey, Reviewer: Joey Scheuer
#Part j - Lead: Tristan Rumsey, Reviewer: Joey Scheuer
#Part k - Lead: Tristan Rumsey, Reviewer: Joey Scheuer




```


#Install needed dependencies
```{r, dependencies, echo=setecho}
# First thing you have to installing the following dependencies within library()
# and you only have to do this once
suppressWarnings({
  library(haven)
  library(readr)
  suppressMessages({library(GGally)})
})
```


## Part a
```{r, part_a, echo=setecho}
url = "https://stats.idre.ucla.edu/wp-content/uploads/2016/02/crime.sas7bdat"
crime_import = read_sas(url, NULL)
str(crime_import)
```
#### Answers and comments:
The dataset has a total of 51 observations, and a total of 9 variables. Of the 9 variables, 8 of them are numeric, and 1 is categorical (state).


## Part b
```{r, part_b, echo=setecho}
write.csv(crime_import, file = "crime_csv.csv", row.names = F)
write.table(crime_import, file = "crime_txt.txt", sep = "\t", row.names = F)
dir()
```
#### Answers and comments:
As seen above, the current working directory has a total of 7 different items, including the csv file of the data that we just created, titled "crime_csv", as well as the text file titled "crime_txt".


## Part c
```{r, part_c, echo=setecho}
crime_numeric = read_csv("crime_csv.csv",
                         col_types = cols(sid = col_skip(),
                                          state = col_skip(),
                                          crime = col_skip()))

str(crime_numeric)
```
#### Answers and comments:
We have removed the three variables called sid (state id), state, and crime, and assigned the remaining 6 variables to the "crime_numeric" dataframe. In the structure of this dataframe, there are 51 observations, and a total of 6 variables, all of which are numeric variables.


## Part d
```{r, part_d, echo=setecho}
boxplot(crime_numeric, las = 2, col = rainbow(ncol(crime_numeric)))
```
#### Answers and comments:
In the above boxplot, all 6 variables can be visualized in a different color. You can identify outliers for several of the variables. This includes 2 outliers for the "murder" variable, 2 outliers for the "pctwhite" variable, and 1 outlier for the "single" variable


## Part e
```{r, part_e, echo=setecho}
ggpairs(crime_numeric)
```
#### Answers and comments:
#Murder variable:
The murder variable has a heavily right-skewed distribution. 
#Pctmetro variable:
The pctmetro variable has a left-skewed distribution.
#Pctwhite variable:
The pctwhite variable has a heavily left-skewed distribution.
#Pcths variable:
The pcths variable has a relatively normal distribution.
#Poverty variable:
The poverty variable has a right-skewed distribution.
#Single variable:
The single variable has a heavily right-skewed distribution.
#Murder and pctmetro variables:
The murder variable has a weak positive association with pctmetro, as the correlation coefficient, r, equals 0.316, which is confirmed by the scatterplot that shows a positive slope.
#Murder and pctwhite variables:
The murder variable has a moderately strong negative association with pctwhite variable, as r = -0.706. This strong correlation is confirmed by the scatterplot that shows a strong relationship.
#Murder and pcths variables:
The murder variable has a moderately weak negative association with the pcths variable, as r = -0.286. This is confirmed by the scatterplot that shows a weak relationship.
#Murder and poverty variables:
The murder variable has a moderately strong positive association with the poverty variable, as r = 0.566. This is confirmed by the scatterplot that shows a moderately strong relationship.
#Murder and single variables:
The murder variable has a strong positive association with the single variable, as r = 0.859. This is confirmed by the scatterplot that shows a strong relationship.
#Pctmetro and pctwhite variables:
The pctmetro variable has a moderately weak negative association with the pctwhite variable, as r = -0.337. This is confirmed by the scatterplot that shows a moderately weak relationship.
#Pctmetro and pcths variables:
The pctmetro variable has a no association with the pcths variable, as r = -0.004. This is confirmed by the scatterplot that shows virtually no relationship.
#Pctmetro and poverty variables:
The pctmetro variable has a very weak negative association with the poverty variable, as r = -0.061. This is confirmed by the scatterplot that shows a very weak relationship.
#Pctmetro and single variables:
The pctmetro variable has a moderately weak positive association with the poverty variable, as r = 0.260. This is confirmed by the scatterplot that shows a moderately weak relationship.
#Pctwhite and pcths variables:
The pctwhite variable has a moderately weak positive association with the pcths variable, as r = 0.339. This is confirmed by the scatterplot that shows a moderately weak relationship.
#Pctwhite and poverty variables:
The pctwhite variable has a moderately weak negative association with the poverty variable, as r = -0.389. This is confirmed by the scatterplot that shows a moderately weak relationship.
#Pctwhite and single variables:
The pctwhite variable has a moderately strong negative association with the poverty variable, as r = -0.656. This is confirmed by the scatterplot that shows a moderately strong relationship.
#Pcths and poverty variables:
The pcths variable has a strong negative association with the poverty variable, as r = -0.744. This is confirmed by the scatterplot that shows a strong relationship.
#Pcths and single variables:
The pcths variable has a weak negative association with the single variable, as r = -0.220. This is confirmed by the scatterplot that shows a weak relationship.
#Poverty and single variables:
The poverty variable has a moderately strong positive association with the single variable, as r = 0.549. This is confirmed by the scatterplot that shows a moderately strong relationship.


## Part f
```{r, part_e, echo=setecho}
#converting data into dataframe and into matrix
crime_df = data.frame(crime_numeric)
crime_matrix = data.matrix(crime_df)
crime_matrix #matrix to use for creating covariance and correlation matrices

#sample covariance matrix
S = round(cov(crime_matrix), 2)
S

#sample correlation matrix
R = round(cor(crime_matrix), 2)
R

#standardized data matrix
std.crime_mat = scale(crime_matrix)

#standardized sample covariance matrix
S.std = round(cov(std.crime_mat), 2)
S.std
```
#### Answers and comments:


## Part g
```{r, part_e, echo=setecho}
#calculate GSV --> >0, so S is invertible
GSV = det(S)
GSV

#inverse of sample covariance matrix
S.inv = round(solve(S), 3)
S.inv
```
#### Answers and comments:


## Part h
```{r, part_e, echo=setecho}
#spectral decomposition
Eig.S = eigen(S)
EigVals.S = round(Eig.S$values, 2)
EigVals.S #getting the eigenvalues

#obtaining the eigenvector matrix (P)
P=round(Eig.S$vectors, 2)
P #creating matrix of normalized eigenvectors

#showing that P'P = PP' = I
round(P %*% t(P), 2) # equals the identity matrix -- correct!

#obtaining the lambda matrix (D)
D=diag(EigVals.S)
D

#compute the spectral decomposition matrix, PDP'
Cov.Spec.Decomp = round(P %*% D %*% t(P), 2)
Cov.Spec.Decomp
S #for comparison to the original sample covariance matrix

#relation b/w the "product matrix and the original sample covariance matrix":
  #The values of the original covariance matrix and the spectral decomposition matrix are very close!

#calculating determinant and inverse of covariance matrix using spectral decomposition
det.S.decomp = prod(EigVals.S) #calculating determinant based on eigenvalues
det.S.decomp #This value is almost the exact same as what we found in part g!

D.inv = round(diag(1/EigVals.S), 2) #calculating lambda inverse matrix (D inverse)
D.inv

S.inv.decomp = P %*% D.inv %*% t(P) #calculating inverse covariance matrix
S.inv.decomp

#Comment on the definiteness of the matrix:
  #A matrix is a PD matrix if all of its eigenvalues > 0. As seen in the beginning of part h, in calculating the eigenvalues,
  #we found eigenvalues of 536.70, 210.98, 43.62, 33.20, 4.09, and 0.97. All of these eigenvalues are greater than 1, so
  # we can assuredly say that the covariance matrix (S) is a positive definite (PD) matrix.
```
<<<<<<< HEAD
#### Answers and comments:
#We can see that creating P by combining the eigenvectors and multiplying that P' results in the identity matrix. When comparing the original covariance matrix and the spectral decomposition matrix we can see that the values of the two matrices are very close. When we calculate the determinant in this step we get 650566462 which is very close to the value of 650566462 that we get when we calculate the determinant in step g. A matrix is a PD matrix if all of its eigenvalues > 0. As seen in the beginning of part h, in calculating the eigenvalues, we found eigenvalues of 536.70, 210.98, 43.62, 33.20, 4.09, and 0.97. All of these eigenvalues are greater than 0. Because of the, we can say that the covariance matrix (S) is a positive definite (PD) matrix.
=======
#### Answers and comments:

>>>>>>> fce4c7e32fd14f39e9963a9d6f11270529878721

## Part i
```{r, part_e, echo=setecho}
#get the inverse square root of the covariance matrix (D^-1/2)
D.inv.sqrt = round(diag(sqrt(1/EigVals.S)),2)
D.inv.sqrt

#multiply the "data matrix" and inverse square root of the covariance matrix
scaled_matrix = round(crime_matrix %*% D.inv.sqrt, 2)

#compute covariance matrix of the scaled data (is "scaled data" what is in line 135?)
S.scaled = round(cov(scaled_matrix), 2)
S.scaled
```
#### Answers and comments:
By taking the inverse square root of sample covariance matrix S, we obtained in part g, we get a matrix filled with some non-real values because some of the covariance have a negative value.
Because of that we end up with a matrix full of na's


## Part j
```{r, part_e, echo=setecho}
#simulate and print the random sample
set.seed(123)
cat("Random vector with n = # feature in crime_mat = 6:\n")
x_vec = rnorm(dim(crime_df)[2], mean=mean(crime_mat), sd=sd(crime_mat))
x_vec
#show S is a PD matrix (x' %*% S %*% x)
t(x_vec) %*% S %*% x_vec #616.6908 --> >0 implies that S is a PD matrix
```
#### Answers and comments:
We simulated from a normal distribution a random sample of size equal to the number of features in crime_mat, and its corresponding sample mean and and standard deviation, but before we set the random seed to 123, to allow us to reproduce the output in different runtime.
Finally, we named this random vector X and we showed that X'SX it returns a very large positive value as expected for a positive definite matrix. In fact, we could theoretically repeat the experiment with all the possible (infinite) combinations of such a vector and still get a positive value.



## Part k
```{r, part_e, echo=setecho}
# Creating the SVD matrix
SVD_matrix = svd(crime_mat)
# Creating the U matrix
U = SVD_matrix$u
# Creating the V matrix
V = SVD_matrix$v
# Creating and printing the lambda matrix
lambda = diag(SVD_matrix$d)
cat("Lambda Matrix from SVD:\n\n")
lambda
# Checking and printing the dimensions of U, V, and Lambda
cat(paste("\nDimension of U:", paste(dim(U), collapse = "x")), "\n")
cat(paste("Dimension of V:", paste(dim(V), collapse = "x")), "\n")
cat(paste("Dimension of Lambda:", paste(dim(lambda), collapse = "x")), "\n")
# Reconstructing original crime_mat data with UDV'
X = U%*%lambda%*%t(V)
# Showing how this reconstruction it returns the original data (-with onlt floating points errors)
cat("\nTotal overall difference between original crime_mat data and reconstructed data matrix with SVD: ",
    max(abs(crime_mat-X)))
r = dim(lambda)[1]-2
lambda_r = lambda[1:r, 1:r]
cat(paste("\n\nNew dimension of Lambda:", paste(dim(lambda_r), collapse = "x")))
U_r = U[, 1:r]
V_r = V[, 1:r]
X_reduced_reconstruct = U_r %*% lambda_r %*% t(V_r)
cat("\n\nTotal overall difference between original crime_mat data and reconstructed and p-2 reduced data matrix with SVD: ", max(abs(crime_mat-X_reduced_reconstruct)))
```
#### Answers and comments:
When computing the singular value decomposition on our crime_mat matrix we can notice that from lambda matrix printout, none of the values is 0 zero or close to zero. Because of that we can infer that none of the variables in crime_mat is a linear combination of other variables, and we should that reducing the rank it will cause a loss of important features
The dimension of matrices U, V and lambda from SVD are respectively 51 by 6, 6 by 6, and 6 by 6.
Moreover, by taking advantage of SVD we proved that the identity X = UDV' (where X is the original data matrix) holds true. In fact, by computing the absolute value of the difference between crime_mat and the matrix obtained by multiplying U, D and V', we obtain a value that is basically zero.
Finally, we wanted to show that by reducing the rank by 2, with features that as showed by the printout of lambda have a non-zero value, we cannot obtain our original crime_mat matrix by UDV' identity.
In showing this we reduced the rank by eliminating the 2 columns with lowest eigenvalues in main diagonal, and by computing the absolute difference between crime_mat and UDV' with reduced rank we get a value of 25.75, showing that the matrix reconstructed with reduced rank without non-zero eigenvalues is different then crime_mat.






